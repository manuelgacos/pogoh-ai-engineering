{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c61996",
   "metadata": {},
   "source": [
    "# Initial data cleaning\n",
    "\n",
    "In this notebook we do some data cleaning for a small portion of the POGOH dataset, this will give some ideas on how to proceed for dealing with the data at a larger scale.\n",
    "\n",
    "__NOTE:__ In this dataset there were some NaN observations in End Station Id and End Station Name, and because of this the ID column is read as a float instead of integer. We handle this issue in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7544b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ecebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47523 entries, 0 to 47522\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Closed Status       47523 non-null  object        \n",
      " 1   Duration            47523 non-null  int64         \n",
      " 2   Start Station Id    47523 non-null  int64         \n",
      " 3   Start Date          47523 non-null  datetime64[ns]\n",
      " 4   Start Station Name  47523 non-null  object        \n",
      " 5   End Date            47523 non-null  datetime64[ns]\n",
      " 6   End Station Id      47497 non-null  float64       \n",
      " 7   End Station Name    47497 non-null  object        \n",
      " 8   Rider Type          47523 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), int64(2), object(4)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "file_path = \"/home/manuel/Documents/AI/pogoh-ai-engineering/data/raw/april-2025.xlsx\"\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "pogoh_df = pd.read_excel(file_path)\n",
    "pogoh_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e7912",
   "metadata": {},
   "source": [
    "I noticed that the names of some columns in the dataset have spaces, which might be easy for reading but while using the data for training models might lead to unexpected behavior. For this reason, I decided to convert them to lower case and convert the spaces to underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f322d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['closed_status', 'duration', 'start_station_id', 'start_date',\n",
       "       'start_station_name', 'end_date', 'end_station_id', 'end_station_name',\n",
       "       'rider_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize column names: lowercase, replace spaces with underscores\n",
    "pogoh_df.columns = (\n",
    "    pogoh_df.columns.str.strip()          # remove leading/trailing spaces\n",
    "                     .str.lower()         # convert to lowercase\n",
    "                     .str.replace(\" \", \"_\")  # replace spaces with underscores\n",
    ")\n",
    "pogoh_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765f580",
   "metadata": {},
   "source": [
    "There are several tasks that could be done for data cleaning, so I'll divide them by broad categories.\n",
    "\n",
    "## Missing & Invalid Data\n",
    "\n",
    "- Inspect and handle rows with missing End Station Id and End Station Name.\n",
    "- Drop or repair rows with Duration <= 0 or End Date < Start Date.\n",
    "- Recalculate duration from timestamps and check against Duration column.\n",
    "\n",
    "### Missing Stations\n",
    "\n",
    "We start by handling trips where either the start station or the end station is missing. In this particular set there were no trips where the starting station information is missing, which makes sense since retrieving the bike from a stations is what initializes a trip. However, this information could still be missing due to some unforseen errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfe33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where either Start Station Id or Start Station Name is missing\n",
    "missing_start_station = pogoh_df[\n",
    "    pogoh_df[\"start_station_id\"].isnull() | pogoh_df[\"start_station_name\"].isnull()\n",
    "]\n",
    "\n",
    "# Count how many of these rows fall into each Closed Status category\n",
    "missing_start_status_counts = missing_start_station[\"closed_status\"].value_counts()\n",
    "print(missing_start_status_counts)\n",
    "\n",
    "# Preview the first few rows with missing values\n",
    "missing_start_status_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d6325",
   "metadata": {},
   "source": [
    "Regarding trips with missing information from the end station, there were 26 trips missing both the end station ID and name. All of them had the closed status terminated.\n",
    "In the case where only one were missing inputing information would be possible by matching the ID or the station name correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7072860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing End Station ID ===\n",
      "26\n",
      "\n",
      "=== Missing End Station Name ===\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18604.961538</td>\n",
       "      <td>32.615385</td>\n",
       "      <td>2025-04-15 08:35:43.500000</td>\n",
       "      <td>2025-04-15 13:45:48.461538304</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>157.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2025-04-03 06:26:13</td>\n",
       "      <td>2025-04-03 06:37:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>480.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2025-04-05 17:44:42.500000</td>\n",
       "      <td>2025-04-05 18:21:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>733.500000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2025-04-14 21:45:04</td>\n",
       "      <td>2025-04-14 22:18:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3437.500000</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>2025-04-21 19:44:15.750000128</td>\n",
       "      <td>2025-04-21 19:47:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>175669.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>2025-04-28 17:22:18</td>\n",
       "      <td>2025-04-28 17:35:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41207.256464</td>\n",
       "      <td>16.613433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            duration  start_station_id                     start_date  \\\n",
       "count      26.000000         26.000000                             26   \n",
       "mean    18604.961538         32.615385     2025-04-15 08:35:43.500000   \n",
       "min       157.000000         10.000000            2025-04-03 06:26:13   \n",
       "25%       480.000000         21.000000     2025-04-05 17:44:42.500000   \n",
       "50%       733.500000         29.000000            2025-04-14 21:45:04   \n",
       "75%      3437.500000         49.250000  2025-04-21 19:44:15.750000128   \n",
       "max    175669.000000         58.000000            2025-04-28 17:22:18   \n",
       "std     41207.256464         16.613433                            NaN   \n",
       "\n",
       "                            end_date  end_station_id  \n",
       "count                             26             0.0  \n",
       "mean   2025-04-15 13:45:48.461538304             NaN  \n",
       "min              2025-04-03 06:37:00             NaN  \n",
       "25%              2025-04-05 18:21:00             NaN  \n",
       "50%              2025-04-14 22:18:00             NaN  \n",
       "75%              2025-04-21 19:47:00             NaN  \n",
       "max              2025-04-28 17:35:00             NaN  \n",
       "std                              NaN             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where either End Station Id or End Station Name is missing\n",
    "missing_end_station = pogoh_df[\n",
    "    pogoh_df[\"end_station_id\"].isnull() | pogoh_df[\"end_station_name\"].isnull()\n",
    "]\n",
    "\n",
    "print(\"=== Missing End Station ID ===\")\n",
    "print(sum(pogoh_df[\"end_station_id\"].isnull()))\n",
    "print(\"\\n=== Missing End Station Name ===\")\n",
    "print(sum(pogoh_df[\"end_station_name\"].isnull()))\n",
    "missing_end_station.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94caeff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed_status\n",
      "TERMINATED    26\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closed_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>rider_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>TERMINATED</td>\n",
       "      <td>762</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-04-28 17:22:18</td>\n",
       "      <td>Ivy St &amp; Walnut St</td>\n",
       "      <td>2025-04-28 17:35:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEMBER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7352</th>\n",
       "      <td>TERMINATED</td>\n",
       "      <td>45506</td>\n",
       "      <td>21</td>\n",
       "      <td>2025-04-27 01:12:34</td>\n",
       "      <td>Liberty Ave &amp; Stanwix St</td>\n",
       "      <td>2025-04-27 13:51:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9308</th>\n",
       "      <td>TERMINATED</td>\n",
       "      <td>78292</td>\n",
       "      <td>21</td>\n",
       "      <td>2025-04-25 17:37:08</td>\n",
       "      <td>Liberty Ave &amp; Stanwix St</td>\n",
       "      <td>2025-04-26 15:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9310</th>\n",
       "      <td>TERMINATED</td>\n",
       "      <td>78398</td>\n",
       "      <td>21</td>\n",
       "      <td>2025-04-25 17:36:22</td>\n",
       "      <td>Liberty Ave &amp; Stanwix St</td>\n",
       "      <td>2025-04-26 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>TERMINATED</td>\n",
       "      <td>78427</td>\n",
       "      <td>21</td>\n",
       "      <td>2025-04-25 17:35:53</td>\n",
       "      <td>Liberty Ave &amp; Stanwix St</td>\n",
       "      <td>2025-04-26 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASUAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     closed_status  duration  start_station_id          start_date  \\\n",
       "4603    TERMINATED       762                15 2025-04-28 17:22:18   \n",
       "7352    TERMINATED     45506                21 2025-04-27 01:12:34   \n",
       "9308    TERMINATED     78292                21 2025-04-25 17:37:08   \n",
       "9310    TERMINATED     78398                21 2025-04-25 17:36:22   \n",
       "9313    TERMINATED     78427                21 2025-04-25 17:35:53   \n",
       "\n",
       "            start_station_name            end_date  end_station_id  \\\n",
       "4603        Ivy St & Walnut St 2025-04-28 17:35:00             NaN   \n",
       "7352  Liberty Ave & Stanwix St 2025-04-27 13:51:00             NaN   \n",
       "9308  Liberty Ave & Stanwix St 2025-04-26 15:22:00             NaN   \n",
       "9310  Liberty Ave & Stanwix St 2025-04-26 15:23:00             NaN   \n",
       "9313  Liberty Ave & Stanwix St 2025-04-26 15:23:00             NaN   \n",
       "\n",
       "     end_station_name rider_type  \n",
       "4603              NaN     MEMBER  \n",
       "7352              NaN     CASUAL  \n",
       "9308              NaN     CASUAL  \n",
       "9310              NaN     CASUAL  \n",
       "9313              NaN     CASUAL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many of these rows fall into each Closed Status category\n",
    "missing_end_status_counts = missing_end_station[\"closed_status\"].value_counts()\n",
    "print(missing_end_status_counts)\n",
    "\n",
    "# Preview the first few rows with missing values\n",
    "missing_end_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ed6e0",
   "metadata": {},
   "source": [
    "The treatment of these kind of trips could be done for an anomaly detection framework. For now, I'll be dropping any trips that might exhibit this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49613f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47523, 9)\n",
      "(47497, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 47497 entries, 0 to 47522\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   closed_status       47497 non-null  object        \n",
      " 1   duration            47497 non-null  int64         \n",
      " 2   start_station_id    47497 non-null  int64         \n",
      " 3   start_date          47497 non-null  datetime64[ns]\n",
      " 4   start_station_name  47497 non-null  object        \n",
      " 5   end_date            47497 non-null  datetime64[ns]\n",
      " 6   end_station_id      47497 non-null  float64       \n",
      " 7   end_station_name    47497 non-null  object        \n",
      " 8   rider_type          47497 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), int64(2), object(4)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing end station ID or name\n",
    "pogoh_df_cleaned = pogoh_df[~pogoh_df[\"end_station_id\"].isnull() & ~pogoh_df[\"end_station_name\"].isnull()].copy()\n",
    "print(pogoh_df.shape)\n",
    "print(pogoh_df_cleaned.shape)\n",
    "pogoh_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed89e39",
   "metadata": {},
   "source": [
    "Since now we've dealt with the observations with missing station IDs, we can convert the columns to integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87556387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 47497 entries, 0 to 47522\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   closed_status       47497 non-null  object        \n",
      " 1   duration            47497 non-null  int64         \n",
      " 2   start_station_id    47497 non-null  int64         \n",
      " 3   start_date          47497 non-null  datetime64[ns]\n",
      " 4   start_station_name  47497 non-null  object        \n",
      " 5   end_date            47497 non-null  datetime64[ns]\n",
      " 6   end_station_id      47497 non-null  int64         \n",
      " 7   end_station_name    47497 non-null  object        \n",
      " 8   rider_type          47497 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(3), object(4)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ensure both start and end station IDs are integers\n",
    "pogoh_df_cleaned[\"start_station_id\"] = pogoh_df_cleaned[\"start_station_id\"].astype(\"int64\")\n",
    "pogoh_df_cleaned[\"end_station_id\"] = pogoh_df_cleaned[\"end_station_id\"].astype(\"int64\")\n",
    "pogoh_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6679e",
   "metadata": {},
   "source": [
    "Now we move to standardizing the names of the stations and checking if the stations IDs map to a unique station. \n",
    "\n",
    "NOTE: There have been instances of POGOH stations being relocated, this might bring some issues. For example, if a station was relocated and the name changed but the ID didn't.\n",
    "\n",
    "For starting stations, all of the ID and names were paired uniquely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ebd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for start station ID mapping to more than one name\n",
    "start_conflicts = (\n",
    "    pogoh_df_cleaned.groupby(\"start_station_id\")[\"start_station_name\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"name_count\")\n",
    ")\n",
    "start_conflicts = start_conflicts[start_conflicts[\"name_count\"] > 1]\n",
    "\n",
    "pogoh_df_cleaned.groupby(\"start_station_id\")[\"start_station_name\"].nunique()\n",
    "\n",
    "# Display actual name mismatches (if any)\n",
    "if not start_conflicts.empty:\n",
    "    display(\n",
    "        pogoh_df_cleaned[\n",
    "            pogoh_df_cleaned[\"start_station_id\"].isin(start_conflicts[\"start_station_id\"])\n",
    "        ][[\"start_station_id\", \"start_station_name\"]].drop_duplicates().sort_values(\"start_station_id\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283cd65",
   "metadata": {},
   "source": [
    "For end stations, the ID and name matching also didn't have any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b34d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_conflicts = (\n",
    "    pogoh_df_cleaned.groupby(\"end_station_id\")[\"end_station_name\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"name_count\")\n",
    ")\n",
    "end_conflicts = end_conflicts[end_conflicts[\"name_count\"] > 1]\n",
    "\n",
    "if not end_conflicts.empty:\n",
    "    display(\n",
    "        pogoh_df_cleaned[\n",
    "            pogoh_df_cleaned[\"end_station_id\"].isin(end_conflicts[\"end_station_id\"])\n",
    "        ][[\"end_station_id\", \"end_station_name\"]].drop_duplicates().sort_values(\"end_station_id\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8db5a",
   "metadata": {},
   "source": [
    "Just to be safe, I'm also checking that the ID/name pairs are exactly the same between the start and end stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11856dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start-only station pairs (not seen in end stations): 0\n",
      "End-only station pairs (not seen in start stations): 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Get unique (station_id, station_name) pairs from start and end columns\n",
    "start_pairs = pogoh_df_cleaned[[\"start_station_id\", \"start_station_name\"]].drop_duplicates()\n",
    "end_pairs = pogoh_df_cleaned[[\"end_station_id\", \"end_station_name\"]].drop_duplicates()\n",
    "\n",
    "# 2. Rename end_pairs to align columns for comparison\n",
    "end_pairs = end_pairs.rename(columns={\n",
    "    \"end_station_id\": \"start_station_id\",\n",
    "    \"end_station_name\": \"start_station_name\"\n",
    "})\n",
    "\n",
    "# 3. Check for pairs that appear in start but not in end\n",
    "start_only = pd.merge(start_pairs, end_pairs, how=\"left\", indicator=True)\n",
    "start_only = start_only[start_only[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "\n",
    "# 4. Check for pairs that appear in end but not in start\n",
    "end_only = pd.merge(end_pairs, start_pairs, how=\"left\", indicator=True)\n",
    "end_only = end_only[end_only[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "\n",
    "# 5. Print result summary\n",
    "print(\"Start-only station pairs (not seen in end stations):\", len(start_only))\n",
    "print(\"End-only station pairs (not seen in start stations):\", len(end_only))\n",
    "\n",
    "# 6. (Optional) Display mismatched pairs if needed\n",
    "if not start_only.empty:\n",
    "    print(\"Start-only mismatches:\")\n",
    "    print(start_only)\n",
    "\n",
    "if not end_only.empty:\n",
    "    print(\"End-only mismatches:\")\n",
    "    print(end_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40f337",
   "metadata": {},
   "source": [
    "All ID/Station Names seem to match. Although this is what was expected. Might be worth doing it in the full dataset as a sanity check. After checking the unique values for names of stations for both end and start stations, we can conclude that along with the names and IDs being the same across start/end stations then there is no cases where the names were misspeled or the IDs were incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d9203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Checking number of unique end station names\n",
    "print(len(pogoh_df_cleaned['end_station_name'].unique()))\n",
    "# Cheking number of unique start station names\n",
    "print(len(pogoh_df_cleaned['start_station_name'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f473ab2",
   "metadata": {},
   "source": [
    "What we do next is format the station names in such a a way that we eliminate all leading or trailing spaces, eliminate/replace all symbols and convert abbreviations to full words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8266f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           schenley drive and schenley drive extension\n",
       "1              north dithridge street and centre avenue\n",
       "2               south bouquet avenue and sennott street\n",
       "3               south bouquet avenue and sennott street\n",
       "4     south 27th street and sidney street southside ...\n",
       "5                  allequippa street and darragh street\n",
       "6                  allequippa street and darragh street\n",
       "7                         42nd street and butler street\n",
       "8                        atwood street and bates street\n",
       "9                     ohara street and university place\n",
       "10              south bouquet avenue and sennott street\n",
       "11                     coltart avenue and forbes avenue\n",
       "12                       atwood street and bates street\n",
       "13              south bouquet avenue and sennott street\n",
       "14                 allequippa street and darragh street\n",
       "Name: start_station_name_clean, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip whitespace at the start and end\n",
    "pogoh_df_cleaned['start_station_name_clean'] = pogoh_df_cleaned['start_station_name'].str.strip()\n",
    "# Make all characters lowercase\n",
    "pogoh_df_cleaned['start_station_name_clean'] = pogoh_df_cleaned['start_station_name_clean'].str.lower()\n",
    "# Normalize inner spaces to just one space\n",
    "pogoh_df_cleaned['start_station_name_clean'] = (\n",
    "    pogoh_df_cleaned['start_station_name_clean'].str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "# Normalize common abbreviations\n",
    "replace_dict = {\n",
    "    r\"\\bst\\b\": \"street\",\n",
    "    r\"\\bave\\b\": \"avenue\",\n",
    "    r\"\\bblvd\\b\": \"boulevard\",\n",
    "    r\"\\bdr\\b\": \"drive\",\n",
    "    r\"\\bext\\b\": \"extension\",\n",
    "    r\"\\bn\\b\": \"north\",\n",
    "    r\"\\bs\\b\": \"south\",\n",
    "    r\"&\": \"and\"\n",
    "}\n",
    "for key, val in replace_dict.items():\n",
    "    pogoh_df_cleaned['start_station_name_clean'] = pogoh_df_cleaned['start_station_name_clean'].str.replace(key, val, regex=True)\n",
    "# Remove punctuation symbols in the strings\n",
    "# NOTE: If done earlier, might erase symbols like & form the names\n",
    "pogoh_df_cleaned['start_station_name_clean'] = (\n",
    "    pogoh_df_cleaned['start_station_name_clean'].str.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    ")\n",
    "\n",
    "\n",
    "pogoh_df_cleaned['start_station_name_clean'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfa2cf",
   "metadata": {},
   "source": [
    "The above procedures will be used repeatedly with other datasets from the POGOH database, so I'll be doing a function implementation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3aa0ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clean_column_names' from 'shared_utils.pogoh_cleaning' (/home/manuel/Documents/AI/pogoh-ai-engineering/shared_utils/pogoh_cleaning.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m/home/manuel/Documents/AI/pogoh-ai-engineering\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpogoh_cleaning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clean_station_names, clean_column_names\n\u001b[32m      5\u001b[39m df_test = pogoh_df.copy()\n\u001b[32m      6\u001b[39m df_test_cleaned =  clean_column_names(df_test)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'clean_column_names' from 'shared_utils.pogoh_cleaning' (/home/manuel/Documents/AI/pogoh-ai-engineering/shared_utils/pogoh_cleaning.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/manuel/Documents/AI/pogoh-ai-engineering')\n",
    "from shared_utils.pogoh_cleaning import clean_station_names, clean_column_names\n",
    "\n",
    "df_test = pogoh_df.copy()\n",
    "df_test_cleaned =  clean_column_names(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "658e25cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84474c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/manuel/Documents/AI/pogoh-ai-engineering/week-0-setup'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pogoh_df.copy()\n",
    "df_test_cleaned =  clean_station_names(df_test, col_name='start_station_name')\n",
    "df_test_cleaned['start_station_name_clean'].head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pogoh_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
